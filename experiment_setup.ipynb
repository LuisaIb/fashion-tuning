{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup: PyTorch, SPOT, RayTune, Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import random \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from ray import tune\n",
    "from ray.air import Checkpoint, session\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.basic_variant import BasicVariantGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 30399), started 6:01:22 ago. (Use '!kill 30399' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-aaa09cfb1e0d8330\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-aaa09cfb1e0d8330\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs\n",
    "\n",
    "# !tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir=\"./data\"):\n",
    "    # transforms\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    # datasets\n",
    "    trainset = torchvision.datasets.FashionMNIST(data_dir,\n",
    "        download=True,\n",
    "        train=True,\n",
    "        transform=transform)\n",
    "    testset = torchvision.datasets.FashionMNIST(data_dir,\n",
    "        download=True,\n",
    "        train=False,\n",
    "        transform=transform)\n",
    "    return trainset, testset\n",
    "\n",
    "trainset, testset = load_data()\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2,\n",
    "                                          worker_init_fn=seed_worker,\n",
    "                                          generator=g)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, \n",
    "                                         batch_size=4,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=2,\n",
    "                                         worker_init_fn=seed_worker,\n",
    "                                         generator=g)\n",
    "\n",
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets, 6000)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1),\n",
    "                    tag=\"big\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                            plot_classes_preds(net, inputs, labels),\n",
    "                            global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. gets the probability predictions in a test_size x num_classes Tensor\n",
    "# 2. gets the preds in a test_size Tensor\n",
    "# takes ~10 seconds to run\n",
    "class_probs = []\n",
    "class_label = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        output = net(images)\n",
    "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
    "\n",
    "        class_probs.append(class_probs_batch)\n",
    "        class_label.append(labels)\n",
    "\n",
    "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "test_label = torch.cat(class_label)\n",
    "\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_truth = test_label == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_truth,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "    writer.close()\n",
    "\n",
    "# plot all the pr curves\n",
    "for i in range(len(classes)):\n",
    "    add_pr_curve_tensorboard(i, test_probs, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RayTune Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fashion_mnist(config, data_dir=None):\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "\n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state[\"epoch\"]\n",
    "        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainset, testset = load_data(data_dir)\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs]\n",
    "    )\n",
    "\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=2, worker_init_fn=seed_worker, generator=g\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=2, worker_init_fn=seed_worker, generator=g\n",
    "    )\n",
    "\n",
    "    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"net_state_dict\": net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
    "\n",
    "        session.report(\n",
    "            {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
    "            checkpoint=checkpoint,\n",
    "        )\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "def test_accuracy(net, device=\"cpu\"):\n",
    "    trainset, testset = load_data()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 09:07:49,138\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-04 09:13:58</td></tr>\n",
       "<tr><td>Running for: </td><td>00:06:08.99        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.7/15.5 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 8.000: -2.330896582921346 | Iter 4.000: -2.3173631292978922 | Iter 2.000: -2.306670645713806 | Iter 1.000: -2.307108553727468<br>Logical resource usage: 8.0/16 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  l1</th><th style=\"text-align: right;\">  l2</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fashion_mnist_9ed5a_00000</td><td>TERMINATED</td><td>172.18.248.14:4168</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">   8</td><td style=\"text-align: right;\">0.0711448</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         365.322</td><td style=\"text-align: right;\">2.3286</td><td style=\"text-align: right;\"> 0.0993333</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26421880 [00:00<?, ?it/s]\n",
      "  0%|          | 32768/26421880 [00:00<03:28, 126760.05it/s]\n",
      "  1%|          | 229376/26421880 [00:00<00:34, 768171.19it/s]\n",
      "  4%|▎         | 983040/26421880 [00:00<00:08, 2853574.72it/s]\n",
      "  7%|▋         | 1867776/26421880 [00:00<00:05, 4624947.08it/s]\n",
      " 10%|█         | 2752512/26421880 [00:00<00:04, 5883967.90it/s]\n",
      " 15%|█▍        | 3866624/26421880 [00:00<00:03, 7461965.14it/s]\n",
      " 18%|█▊        | 4849664/26421880 [00:00<00:02, 8120341.17it/s]\n",
      " 23%|██▎       | 6193152/26421880 [00:01<00:02, 9357032.11it/s]\n",
      " 29%|██▉       | 7634944/26421880 [00:01<00:01, 10744152.38it/s]\n",
      " 35%|███▍      | 9175040/26421880 [00:01<00:01, 11613989.05it/s]\n",
      " 41%|████      | 10780672/26421880 [00:01<00:01, 12721354.95it/s]\n",
      " 46%|████▌     | 12091392/26421880 [00:01<00:01, 12311330.64it/s]\n",
      " 52%|█████▏    | 13795328/26421880 [00:01<00:00, 13629189.99it/s]\n",
      " 60%|█████▉    | 15826944/26421880 [00:01<00:00, 15550409.39it/s]\n",
      " 68%|██████▊   | 17858560/26421880 [00:01<00:00, 16924867.62it/s]\n",
      " 76%|███████▌  | 19988480/26421880 [00:01<00:00, 18161040.18it/s]\n",
      " 83%|████████▎ | 22052864/26421880 [00:01<00:00, 18812274.36it/s]\n",
      " 91%|█████████ | 24018944/26421880 [00:02<00:00, 18941027.77it/s]\n",
      "100%|██████████| 26421880/26421880 [00:02<00:00, 12208480.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 772173.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4422102 [00:00<?, ?it/s]\n",
      "  3%|▎         | 131072/4422102 [00:00<00:03, 1282369.88it/s]\n",
      " 24%|██▎       | 1048576/4422102 [00:00<00:00, 5647652.95it/s]\n",
      " 64%|██████▍   | 2850816/4422102 [00:00<00:00, 11072539.94it/s]\n",
      "100%|██████████| 4422102/4422102 [00:00<00:00, 11263220.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 62405424.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m \n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [1,  2000] loss: 2.315\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [1,  4000] loss: 1.162\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [1,  6000] loss: 0.775\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">  loss</th><th>should_checkpoint  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_fashion_mnist_9ed5a_00000</td><td style=\"text-align: right;\"> 0.0993333</td><td style=\"text-align: right;\">2.3286</td><td>True               </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [2,  2000] loss: 2.322\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [2,  4000] loss: 1.160\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [2,  6000] loss: 0.774\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [3,  2000] loss: 2.322\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [3,  4000] loss: 1.160\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [3,  6000] loss: 0.775\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [4,  2000] loss: 2.321\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [4,  4000] loss: 1.161\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [4,  6000] loss: 0.774\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [5,  2000] loss: 2.323\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [5,  4000] loss: 1.160\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [5,  6000] loss: 0.774\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [6,  2000] loss: 2.322\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [6,  4000] loss: 1.160\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [6,  6000] loss: 0.775\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [7,  2000] loss: 2.323\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [7,  4000] loss: 1.161\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [7,  6000] loss: 0.774\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [8,  2000] loss: 2.322\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [8,  4000] loss: 1.162\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [8,  6000] loss: 0.774\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [9,  2000] loss: 2.324\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [9,  4000] loss: 1.162\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [9,  6000] loss: 0.774\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [10,  2000] loss: 2.323\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [10,  4000] loss: 1.162\n",
      "\u001b[2m\u001b[36m(func pid=4168)\u001b[0m [10,  6000] loss: 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 09:13:58,162\tINFO tune.py:1148 -- Total run time: 369.02 seconds (368.98 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"l1\": tune.choice([2 ** i for i in range(9)]),\n",
    "    \"l2\": tune.choice([2 ** i for i in range(9)]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([2, 4, 8, 16])\n",
    "}\n",
    "\n",
    "\n",
    "max_num_epochs = 10\n",
    "num_samples = 2\n",
    "\n",
    "# Async Hyperband Scheduler\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=max_num_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    ")\n",
    "\n",
    "result = tune.run(\n",
    "    partial(train_fashion_mnist, data_dir=\"./data\"),\n",
    "    resources_per_trial={\"cpu\": 8, \"gpu\": 0},\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    storage_path='./tune_runs/',\n",
    "    search_alg=BasicVariantGenerator(random_state=42),\n",
    "    scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'l1': 64, 'l2': 8, 'lr': 0.07114476009343416, 'batch_size': 8}\n",
      "Best trial final validation loss: 2.3285953847567242\n",
      "Best trial final validation accuracy: 0.09933333333333333\n",
      "Best trial test set accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "device = \"cpu\"\n",
    "best_trained_model.to(device)\n",
    "\n",
    "best_checkpoint = best_trial.checkpoint.to_air_checkpoint()\n",
    "best_checkpoint_data = best_checkpoint.to_dict()\n",
    "\n",
    "best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
    "\n",
    "test_acc = test_accuracy(best_trained_model, device)\n",
    "print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPOT Hyperaprameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotPython.utils.init import fun_control_init\n",
    "from spotPython.utils.file import get_experiment_name, get_spot_tensorboard_path\n",
    "from spotPython.utils.device import getDevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIME = 1 # at least 60 for productive\n",
    "INIT_SIZE = 5 # at least 10 for productive\n",
    "DEVICE = \"auto\" # \"cpu\", \"cuda:0\"\n",
    "PREFIX = \"FashionMNIST\"\n",
    "DEVICE = getDevice(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = get_experiment_name(prefix=PREFIX)\n",
    "\n",
    "fun_control = fun_control_init(\n",
    "    task=\"classification\",\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    device=DEVICE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_data()\n",
    "fun_control.update({\n",
    "    \"train\": train,\n",
    "    \"test\": test,\n",
    "    \"n_samples\": len(train)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
